{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(45)\n",
    "num_samples = 40\n",
    "\n",
    "# Generate data\n",
    "x1 = np.random.uniform(-1, 1, num_samples)\n",
    "f_x = 3*x1 + 4\n",
    "eps = np.random.randn(num_samples)\n",
    "y = f_x + eps\n",
    "\n",
    "x = torch.tensor(x1, dtype=torch.float32, requires_grad=False)\n",
    "y = torch.tensor(y, dtype=torch.float32, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1\n",
    "\n",
    "Finding true gradient on the given data using torch.autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True gradient value of gradient of theta 0: tensor([-5.6164])\n",
      "True gradient value of gradient of theta 1: tensor([-0.5630])\n"
     ]
    }
   ],
   "source": [
    "#declaring random vector\n",
    "\n",
    "def val(theta1, theta0):\n",
    "    theta0 = torch.tensor([float(theta0)], requires_grad=True)\n",
    "    theta1 = torch.tensor([float(theta1)], requires_grad=True)\n",
    "\n",
    "    global x,y\n",
    "\n",
    "    loss = ((y - theta1*x-theta0)**2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    return theta0.grad, theta1.grad\n",
    "\n",
    "t0grad, t1grad = val(1.0, 1.0)\n",
    "\n",
    "print(f\"True gradient value of gradient of theta 0: {t0grad}\")\n",
    "print(f\"True gradient value of gradient of theta 1: {t1grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = torch.tensor([1.0], requires_grad=True)\n",
    "theta1 = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Question2****\n",
    "\n",
    "In this question, we calculate the value of gradient over each point using loss.backward(), which stores the value of gradient in theta1.grad and theta0.grad\n",
    "\n",
    "Upon storing each value, when we take the average value, we get the value that we obtain using the method true gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_theta_0 = []\n",
    "gradients_theta_1 = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    #need to set the value to zero, so that the previously stored data doesnot deviate the calculation\n",
    "    if(i != 0): #excluding the first iterationqq\n",
    "        theta0.grad.zero_()\n",
    "        theta1.grad.zero_()\n",
    "\n",
    "    loss_indi = ((y[i] - theta1*x[i]-theta0)**2)\n",
    "    loss_indi.backward()\n",
    "\n",
    "    gradients_theta_0.append(theta0.grad.item())\n",
    "    gradients_theta_1.append(theta1.grad.item())\n",
    "\n",
    "avg_SDG_theta0 = np.mean(gradients_theta_0)\n",
    "avg_SDG_theta1 = np.mean(gradients_theta_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad. desc values for all the points:\n",
      "Theta0 = -8.05: Theta1 = -7.8731\n",
      "Theta0 = -8.6183: Theta1 = -0.854\n",
      "Theta0 = -3.8722: Theta1 = 1.6926\n",
      "Theta0 = -5.1751: Theta1 = 4.3752\n",
      "Theta0 = -4.4476: Theta1 = 0.494\n",
      "Theta0 = -6.4911: Theta1 = 0.353\n",
      "Theta0 = -1.5076: Theta1 = 1.3613\n",
      "Theta0 = -2.4577: Theta1 = 1.6549\n",
      "Theta0 = -3.3838: Theta1 = 2.5991\n",
      "Theta0 = -8.1772: Theta1 = -2.0834\n",
      "Theta0 = -7.1411: Theta1 = -5.087\n",
      "Theta0 = -5.5181: Theta1 = -1.6566\n",
      "Theta0 = -8.6144: Theta1 = -8.4545\n",
      "Theta0 = -5.469: Theta1 = 0.3243\n",
      "Theta0 = -4.1224: Theta1 = -0.9753\n",
      "Theta0 = -3.7202: Theta1 = 1.6171\n",
      "Theta0 = -12.0666: Theta1 = -11.4875\n",
      "Theta0 = -4.704: Theta1 = -1.6282\n",
      "Theta0 = -5.0257: Theta1 = 0.5977\n",
      "Theta0 = -7.8445: Theta1 = 3.2996\n",
      "Theta0 = -4.8937: Theta1 = -0.0949\n",
      "Theta0 = -1.0841: Theta1 = 0.8402\n",
      "Theta0 = -4.3592: Theta1 = 2.3805\n",
      "Theta0 = -5.8193: Theta1 = 0.2496\n",
      "Theta0 = -5.6503: Theta1 = 2.907\n",
      "Theta0 = -1.9239: Theta1 = 0.431\n",
      "Theta0 = -8.6631: Theta1 = -5.5249\n",
      "Theta0 = -2.8081: Theta1 = 2.3894\n",
      "Theta0 = -9.2035: Theta1 = -7.7889\n",
      "Theta0 = -2.3714: Theta1 = 1.3048\n",
      "Theta0 = -8.0584: Theta1 = -3.326\n",
      "Theta0 = -5.2906: Theta1 = 4.1207\n",
      "Theta0 = -6.9767: Theta1 = -1.4093\n",
      "Theta0 = -5.5822: Theta1 = 1.0406\n",
      "Theta0 = -9.4421: Theta1 = -6.3608\n",
      "Theta0 = -3.5464: Theta1 = 1.773\n",
      "Theta0 = -4.3194: Theta1 = 0.3651\n",
      "Theta0 = -7.0877: Theta1 = -0.8141\n",
      "Theta0 = -6.9844: Theta1 = 3.465\n",
      "Theta0 = -4.1863: Theta1 = 3.2633\n"
     ]
    }
   ],
   "source": [
    "#printing all the values\n",
    "\n",
    "print(f\"Grad. desc values for all the points:\")\n",
    "\n",
    "for a in range(len(x)):\n",
    "    print(f\"Theta0 = {round(gradients_theta_0[a],4)}: Theta1 = {round(gradients_theta_1[a],4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.616434812545776\n",
      "-0.5629974454641342\n"
     ]
    }
   ],
   "source": [
    "print(avg_SDG_theta0)\n",
    "print(avg_SDG_theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we, can see that upon taking the average of all the gradient values of theta0 and theta1 individually upon each point is equal to the gradient equal to the gradient obtained using true gradient or full batch gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Question 3***\n",
    "\n",
    "Comparing the epochs required by various method of gradient descent.\n",
    "\n",
    "The epochs run until the final loss (latest loss) is sufficiently close to the value of the loss at the optimal value of theta vector, that is at the points, that we have obtained using the direct method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing full batch gradient descent**\n",
    "\n",
    "in this the values are updated after each iteration once, that is after each epoch is completed, we update the value of theta vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, Calculating **true gradient**, until convergence value achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding one to as the first row.\n",
    "x_new = torch.stack([torch.ones_like(x), x], dim=1) #creating x vector with 1s and x values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the true value to find the value of actual theta0 and theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, we first calculate the optimum value of theta vector using the standard value. Then we calculate the optimum loss, that is plugging in these value inside the loss function. Then iterating thru each loop and breaking when the current loss - optimum loss converge withing the range of epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.9507, 2.6825])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/_kpfspls6xq0pzzm_dl4ylxc0000gn/T/ipykernel_14043/3607695965.py:4: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  theta = xtx_inverse @ x_new.T @ y.numpy()\n"
     ]
    }
   ],
   "source": [
    "#calculating the optimal theta values\n",
    "\n",
    "xtx_inverse = np.linalg.inv(x_new.T @ x_new)\n",
    "theta = xtx_inverse @ x_new.T @ y.numpy()\n",
    "print(theta)\n",
    "theta0 = theta0_plot = theta[0]\n",
    "theta1 = theta1_plot =theta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta0 (intercept): 3.9507062435150146\n",
      "theta1 (slope): 2.682469367980957\n",
      "Loss value: 0.5957541465759277\n"
     ]
    }
   ],
   "source": [
    "print(f\"theta0 (intercept): {theta0}\")\n",
    "print(f\"theta1 (slope): {theta1}\")\n",
    "\n",
    "#plugging the optimal value of theta0 and theta1 gives us the optimal loss, that is also the minimum loss, as in linear regression we are trying to minimize the loss function, and there is only one minimum point.\n",
    "loss_optimal = ((y - theta1*x-theta0)**2).mean()\n",
    "print(f\"Loss value: {loss_optimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameters as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting some hyperparamaters\n",
    "\n",
    "alpha = 0.0005\n",
    "convergence_threshold = 0.001 #given in question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating **True Gradient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = [] #this is to store all the lists that the function returns for different and random values of theta0 and theta1.\n",
    "average_iter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following part has to be changed, as to initialize it randomly\n",
    "#Finding the average number of iterations using three epoch values\n",
    "\n",
    "\n",
    "#setting the first value of theta0 and theta1 as the following.\n",
    "#for the first case setting a proper value (which is random for the system, but for the sake of the question, it is set to 2.5, for comparison in plotting)\n",
    "\n",
    "theta0_init = 2.5\n",
    "theta1_init = 2.5\n",
    "theta0 = torch.tensor([theta0_init], requires_grad=True) \n",
    "theta1 = torch.tensor([theta1_init], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function, which returns three lists -> theta0, theta1 and loss after each epoch.\n",
    "\n",
    "Inside the function, there is a while loop, which is set true, and it breaks only when, the current loss function, is epsilon close to the optimal theta value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def true_gradient(theta0, theta1):\n",
    "    theta0 = torch.tensor([float(theta0)], requires_grad=True)\n",
    "    theta1 = torch.tensor([float(theta1)], requires_grad=True)\n",
    "\n",
    "    global x,y, alpha, convergence_threshold, theta0_plot, theta1_plot, loss_optimal\n",
    "    val_theta0 = [theta0.item()]\n",
    "    val_theta1 = [theta1.item()]\n",
    "    loss_values = [loss_optimal.item()]\n",
    "\n",
    "    epoch_vanilla = 0\n",
    "    epoch_f = 200000 #maximum number of epochs, just to contraint if there is no convergence.\n",
    "\n",
    "    while epoch_vanilla < epoch_f:\n",
    "        \n",
    "        loss = ((y - theta1 * x - theta0) ** 2).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            theta0 -= alpha * theta0.grad\n",
    "            theta1 -= alpha * theta1.grad\n",
    "        \n",
    "        val_theta0.append(theta0.detach().item())\n",
    "        val_theta1.append(theta1.detach().item())\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        distance = torch.sqrt((theta0 - theta0_plot) ** 2 + (theta1 - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "        epoch_vanilla += 1\n",
    "\n",
    "        \n",
    "        theta0.grad.zero_()\n",
    "        theta1.grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"Converged after {epoch_vanilla} epochs\")\n",
    "    print(f\"Final theta0: {theta0.item()}\")\n",
    "    print(f\"Final theta1: {theta1.item()}\")\n",
    "    return val_theta0, val_theta1, loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 19500 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n"
     ]
    }
   ],
   "source": [
    "#giving theta0 and theta1 as the input to the function, and then the three lists obtained are stored in a list called finallist.\n",
    "\n",
    "val_theta0, val_theta1, loss_values = true_gradient(theta0_init, theta1_init)\n",
    "final_list.append([val_theta0, val_theta1, loss_values])\n",
    "average_iter.append(len(val_theta0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def true_gradient(theta0, theta1):\n",
    "#     theta0 = torch.tensor([float(theta0)], requires_grad=True)\n",
    "#     theta1 = torch.tensor([float(theta1)], requires_grad=True)\n",
    "\n",
    "#     global x,y, alpha, convergence_threshold\n",
    "#     val_theta0 = [theta0.item()]\n",
    "#     val_theta1 = [theta1.item()]\n",
    "#     loss_values = [loss_optimal.item()]\n",
    "\n",
    "#     epoch_vanilla = 0\n",
    "#     epoch_f = 200000 #maximum number of epochs, just to contraint if there is no convergence.\n",
    "\n",
    "#     while epoch_vanilla < epoch_f:\n",
    "        \n",
    "#         loss = ((y - theta1 * x - theta0) ** 2).mean()\n",
    "#         loss.backward()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "        \n",
    "#             theta0 -= alpha * theta0.grad\n",
    "#             theta1 -= alpha * theta1.grad\n",
    "        \n",
    "#         val_theta0.append(theta0.detach().item())\n",
    "#         val_theta1.append(theta1.detach().item())\n",
    "#         loss_values.append(loss.item())\n",
    "\n",
    "        \n",
    "#         theta0.grad.zero_()\n",
    "#         theta1.grad.zero_()\n",
    "\n",
    "#         if (abs(loss_optimal - loss) < convergence_threshold):\n",
    "#             break\n",
    "#         epoch_vanilla += 1\n",
    "\n",
    "#     print(f\"Converged after {epoch_vanilla} epochs\")\n",
    "#     print(f\"Final theta0: {theta0.item()}\")\n",
    "#     print(f\"Final theta1: {theta1.item()}\")\n",
    "#     return val_theta0, val_theta1, loss_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending the data of first random value of theta0 and theta1 as a CSV file for plotting and visualization\n",
    "data_vanilla = pd.DataFrame({\n",
    "    'val_theta0': val_theta0,\n",
    "    'val_theta1': val_theta1,\n",
    "    'loss_values': loss_values\n",
    "})\n",
    "data_vanilla.to_csv('processed_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the trial for second random value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random values\n",
    "theta0_rand_values = np.random.uniform(0, 5, 10)\n",
    "theta0_rand_values = np.random.uniform(0, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 26352 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 21294 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 25698 epochs\n",
      "Final theta0: 3.9509084224700928\n",
      "Final theta1: 2.683448553085327\n",
      "Converged after 24938 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 20643 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 26128 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 25519 epochs\n",
      "Final theta0: 3.9509084224700928\n",
      "Final theta1: 2.683448553085327\n",
      "Converged after 22424 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n",
      "Converged after 26439 epochs\n",
      "Final theta0: 3.9505045413970947\n",
      "Final theta1: 2.681490182876587\n"
     ]
    }
   ],
   "source": [
    "for a in range(9):\n",
    "    theta0 = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    theta1 = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    val_theta0, val_theta1, loss_values = true_gradient(theta0_rand_values[a], theta0_rand_values[a])\n",
    "    final_list.append([val_theta0, val_theta1, loss_values])\n",
    "    average_iter.append(len(val_theta0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the average number of iterations required for the convergence of the loss function.\n",
    "Here the number of iterations equal to the number of epochs required for the convergence of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of epochs/iteration: 23895.5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of epochs/iteration: {np.mean(average_iter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD method for the epoch calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_sgd = 0.0001\n",
    "convergence_threshold = 0.001\n",
    "epoch_f_sgd = 50000\n",
    "sgd_avg_iteration = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0_sgd = torch.tensor([2.5], requires_grad=True)\n",
    "theta1_sgd = torch.tensor([2.5], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(theta0_sgd, theta1_sgd,x, y, alpha_sgd, convergence_threshold, loss_optimal, epoch_f_sgd):\n",
    "\n",
    "    val_theta0_sgd = [theta0_sgd.item()]\n",
    "    val_theta1_sgd = [theta1_sgd.item()]\n",
    "    loss_values_sgd = [loss_optimal.item()] \n",
    "\n",
    "    epoch_sgd = 0\n",
    "\n",
    "    flag = True\n",
    "\n",
    "    while epoch_sgd < epoch_f_sgd and flag:\n",
    "        indices = torch.randperm(x.size(0))\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "        for i in range(len(x)):\n",
    "            prediction = theta1_sgd * x[i] + theta0_sgd\n",
    "            loss_sgd = (y[i] - prediction) ** 2\n",
    "            loss_sgd.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                theta0_sgd -= alpha_sgd * theta0_sgd.grad\n",
    "                theta1_sgd -= alpha_sgd * theta1_sgd.grad\n",
    "\n",
    "            theta0_sgd.grad.zero_()\n",
    "            theta1_sgd.grad.zero_()\n",
    "\n",
    "        total_loss_sgd = ((y - (theta1_sgd * x + theta0_sgd)) ** 2).mean()\n",
    "\n",
    "        distance = torch.sqrt((theta0_sgd - theta0_plot) ** 2 + (theta1_sgd - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "\n",
    "        val_theta0_sgd.append(theta0_sgd.detach().item())\n",
    "        val_theta1_sgd.append(theta1_sgd.detach().item())\n",
    "        loss_values_sgd.append(total_loss_sgd.item())\n",
    "\n",
    "        epoch_sgd += 1\n",
    "\n",
    "    print(f\"Converged after {epoch_sgd} epochs\")\n",
    "    print(f\"Final theta0: {theta0_sgd.item()}\")\n",
    "    print(f\"Final theta1: {theta1_sgd.item()}\")\n",
    "    print(f\"Final loss: {loss_values_sgd[-1]}\")\n",
    "    return val_theta0_sgd, val_theta1_sgd, loss_values_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sgd(theta0_sgd, theta1_sgd,x, y, alpha_sgd, convergence_threshold, loss_optimal, epoch_f_sgd):\n",
    "\n",
    "#     val_theta0_sgd = [theta0_sgd.item()]\n",
    "#     val_theta1_sgd = [theta1_sgd.item()]\n",
    "#     loss_values_sgd = [loss_optimal.item()] \n",
    "\n",
    "#     epoch_sgd = 0\n",
    "\n",
    "#     flag = True\n",
    "\n",
    "#     while epoch_sgd < epoch_f_sgd and flag:\n",
    "        \n",
    "#         for i in range(len(x)):\n",
    "#             prediction = theta1_sgd * x[i] + theta0_sgd\n",
    "#             loss_sgd = (y[i] - prediction) ** 2\n",
    "#             loss_sgd.backward()\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 theta0_sgd -= alpha_sgd * theta0_sgd.grad\n",
    "#                 theta1_sgd -= alpha_sgd * theta1_sgd.grad\n",
    "\n",
    "#             theta0_sgd.grad.zero_()\n",
    "#             theta1_sgd.grad.zero_()\n",
    "\n",
    "#         total_loss_sgd = ((y - (theta1_sgd * x + theta0_sgd)) ** 2).mean()\n",
    "\n",
    "#         if (abs(loss_optimal - loss_sgd) < convergence_threshold):\n",
    "#             break\n",
    "\n",
    "#         val_theta0_sgd.append(theta0_sgd.detach().item())\n",
    "#         val_theta1_sgd.append(theta1_sgd.detach().item())\n",
    "#         loss_values_sgd.append(total_loss_sgd.item())\n",
    "\n",
    "#         epoch_sgd += 1\n",
    "\n",
    "#     print(f\"Converged after {epoch_sgd} epochs\")\n",
    "#     print(f\"Final theta0: {theta0_sgd.item()}\")\n",
    "#     print(f\"Final theta1: {theta1_sgd.item()}\")\n",
    "#     print(f\"Final loss: {loss_values_sgd[-1]}\")\n",
    "#     return val_theta0_sgd, val_theta1_sgd, loss_values_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2344 epochs\n",
      "Final theta0: 3.95086932182312\n",
      "Final theta1: 2.681483745574951\n",
      "Final loss: 0.5957545638084412\n"
     ]
    }
   ],
   "source": [
    "val_theta0_sgd, val_theta1_sgd, loss_values_sgd = sgd(theta0_sgd, theta1_sgd, x, y, alpha_sgd, convergence_threshold, loss_optimal, epoch_f_sgd)\n",
    "sgd_avg_iteration.append(len(val_theta0_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sgd = pd.DataFrame({\n",
    "    'val_theta0_sgd': val_theta0_sgd,\n",
    "    'val_theta1_sgd': val_theta1_sgd,\n",
    "    'loss_values_sgd': loss_values_sgd\n",
    "})\n",
    "data_sgd.to_csv('processed_data_sgd.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 3234 epochs\n",
      "Final theta0: 3.95084036882625\n",
      "Final theta1: 2.6814802044671175\n",
      "Final loss: 0.595754528851232\n",
      "Converged after 2602 epochs\n",
      "Final theta0: 3.9508404363507594\n",
      "Final theta1: 2.6814807672804513\n",
      "Final loss: 0.5957545285147067\n",
      "Converged after 3236 epochs\n",
      "Final theta0: 3.95107447144578\n",
      "Final theta1: 2.683398119514469\n",
      "Final loss: 0.5957545296218004\n",
      "Converged after 3057 epochs\n",
      "Final theta0: 3.95084028529229\n",
      "Final theta1: 2.681479520376889\n",
      "Final loss: 0.5957545292598979\n",
      "Converged after 2520 epochs\n",
      "Final theta0: 3.950840233432425\n",
      "Final theta1: 2.6814791129855107\n",
      "Final loss: 0.5957545295024788\n",
      "Converged after 3206 epochs\n",
      "Final theta0: 3.950840383441109\n",
      "Final theta1: 2.681480324225685\n",
      "Final loss: 0.595754528779716\n",
      "Converged after 3214 epochs\n",
      "Final theta0: 3.951074389099826\n",
      "Final theta1: 2.6833974448842186\n",
      "Final loss: 0.5957545292185148\n",
      "Converged after 2743 epochs\n",
      "Final theta0: 3.9508403605647255\n",
      "Final theta1: 2.6814801401559403\n",
      "Final loss: 0.5957545288894588\n",
      "Converged after 3245 epochs\n",
      "Final theta0: 3.9508404205584515\n",
      "Final theta1: 2.6814806282811467\n",
      "Final loss: 0.595754528598189\n",
      "Converged after 3209 epochs\n",
      "Final theta0: 3.950840244729102\n",
      "Final theta1: 2.681479187807357\n",
      "Final loss: 0.5957545294586962\n"
     ]
    }
   ],
   "source": [
    "#doing the same for random values of theta0 and theta1\n",
    "for a in range(len(theta0_rand_values)):\n",
    "    theta0_sgd = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    theta1_sgd = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    val_theta0_sgd, val_theta1_sgd, loss_values_sgd = sgd(theta0_sgd, theta1_sgd, x, y, alpha_sgd, convergence_threshold, loss_optimal, epoch_f_sgd)\n",
    "    sgd_avg_iteration.append(len(val_theta0_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the average number of epochs required for convergence of the loss function.\n",
    "Here the average number of iterations required is equal to the number of epochs x number of elements in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of epochs for SGD: 2965.5454545454545\n",
      "Average number of iterations for SGD: 118621.81818181818\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of epochs for SGD: {np.mean(sgd_avg_iteration)}\")\n",
    "print(f\"Average number of iterations for SGD: {np.mean(sgd_avg_iteration)*len(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent using Mini-Batch method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mini_batch_gradient_descent(x, y, theta0_mini_batch, theta1_mini_batch, alpha, batch_size, convergence_threshold, epoch_f_mini_batch):\n",
    "\n",
    "    val_theta0_mini_batch = [theta0_mini_batch.item()]\n",
    "    val_theta1_mini_batch = [theta1_mini_batch.item()]\n",
    "    loss_values_mini_batch = [loss_optimal.item()]\n",
    "    global alpha_mini_batch, theta0_plot, theta1_plot\n",
    "    epoch_mini_batch = 0\n",
    "    prev_loss = 0\n",
    "\n",
    "    while (epoch_mini_batch < epoch_f_mini_batch):\n",
    "        indices = torch.randperm(x.size(0))\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        for i in range(0, x.size()[0], batch_size):\n",
    "            batch_x = x[i:i + batch_size]\n",
    "            batch_y = y[i: i + batch_size]\n",
    "\n",
    "            loss_mini_batch = ((batch_y - theta1_mini_batch * batch_x - theta0_mini_batch) ** 2).mean()\n",
    "            loss_mini_batch.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                theta0_mini_batch -= alpha_mini_batch * theta0_mini_batch.grad\n",
    "                theta1_mini_batch -= alpha_mini_batch * theta1_mini_batch.grad\n",
    "\n",
    "            theta0_mini_batch.grad.zero_()\n",
    "            theta1_mini_batch.grad.zero_()\n",
    "\n",
    "        total_loss_mini_batch = ((y - (theta1_mini_batch * x + theta0_mini_batch)) ** 2).mean()\n",
    "\n",
    "        val_theta0_mini_batch.append(theta0_mini_batch.detach().item())\n",
    "        val_theta1_mini_batch.append(theta1_mini_batch.detach().item())\n",
    "        loss_values_mini_batch.append(total_loss_mini_batch.item())\n",
    "\n",
    "        distance = torch.sqrt((theta0_mini_batch - theta0_plot) ** 2 + (theta1_mini_batch - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "\n",
    "        epoch_mini_batch += 1\n",
    "\n",
    "    print(f\"Converged after {epoch_mini_batch} epochs\")\n",
    "    print(f\"Final theta0: {theta0_mini_batch.item()}\")\n",
    "    print(f\"Final theta1: {theta1_mini_batch.item()}\")\n",
    "    return val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def mini_batch_gradient_descent(x, y, theta0_mini_batch, theta1_mini_batch, alpha, batch_size, convergence_threshold, epoch_f_mini_batch):\n",
    "\n",
    "#     val_theta0_mini_batch = [theta0_mini_batch.item()]\n",
    "#     val_theta1_mini_batch = [theta1_mini_batch.item()]\n",
    "#     loss_values_mini_batch = [loss_optimal.item()]\n",
    "\n",
    "#     epoch_mini_batch = 0\n",
    "#     prev_loss = 0\n",
    "\n",
    "#     while (epoch_mini_batch < epoch_f_mini_batch):\n",
    "#         indices = torch.randperm(x.size(0))\n",
    "#         x = x[indices]\n",
    "#         y = y[indices]\n",
    "\n",
    "#         for i in range(0, x.size()[0], batch_size):\n",
    "#             batch_x = x[i:i + batch_size]\n",
    "#             batch_y = y[i: i + batch_size]\n",
    "\n",
    "#             loss_mini_batch = ((batch_y - theta1_mini_batch * batch_x - theta0_mini_batch) ** 2).mean()\n",
    "#             loss_mini_batch.backward()\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 theta0_mini_batch -= alpha_mini_batch * theta0_mini_batch.grad\n",
    "#                 theta1_mini_batch -= alpha_mini_batch * theta1_mini_batch.grad\n",
    "\n",
    "#             theta0_mini_batch.grad.zero_()\n",
    "#             theta1_mini_batch.grad.zero_()\n",
    "\n",
    "#         total_loss_mini_batch = ((y - (theta1_mini_batch * x + theta0_mini_batch)) ** 2).mean()\n",
    "\n",
    "#         val_theta0_mini_batch.append(theta0_mini_batch.detach().item())\n",
    "#         val_theta1_mini_batch.append(theta1_mini_batch.detach().item())\n",
    "#         loss_values_mini_batch.append(total_loss_mini_batch.item())\n",
    "\n",
    "#         if abs(total_loss_mini_batch - loss_optimal) < convergence_threshold:\n",
    "#             break\n",
    "\n",
    "#         epoch_mini_batch += 1\n",
    "\n",
    "#     print(f\"Converged after {epoch_mini_batch} epochs\")\n",
    "#     print(f\"Final theta0: {theta0_mini_batch.item()}\")\n",
    "#     print(f\"Final theta1: {theta1_mini_batch.item()}\")\n",
    "#     return val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "alpha_mini_batch = 0.0001\n",
    "convergence_threshold = 0.001\n",
    "epoch_f_mini_batch = 100000\n",
    "\n",
    "#for the first iteration, setting the values of theta0 and theta1 as 2.5, for comparison in plotting\n",
    "theta0_mini_batch = torch.tensor([2.5], requires_grad=True)\n",
    "theta1_mini_batch = torch.tensor([3.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 20249 epochs\n",
      "Final theta0: 3.9508249759674072\n",
      "Final theta1: 2.683461904525757\n"
     ]
    }
   ],
   "source": [
    "val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch = mini_batch_gradient_descent(x, y, theta0_mini_batch, theta1_mini_batch, alpha, batch_size, convergence_threshold, epoch_f_mini_batch)\n",
    "mini_batch_iterations.append(len(val_theta0_mini_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "data_vanilla = pd.DataFrame({\n",
    "    'val_theta0': val_theta0_mini_batch,\n",
    "    'val_theta1': val_theta1_mini_batch,\n",
    "    'loss_values_mini_batch': loss_values_mini_batch\n",
    "})\n",
    "data_vanilla.to_csv('processed_data_mini_batch.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 32609 epochs\n",
      "Final theta0: 3.9505859613299554\n",
      "Final theta1: 2.681476686604201\n",
      "Converged after 26286 epochs\n",
      "Final theta0: 3.9505857303611154\n",
      "Final theta1: 2.681476807112101\n",
      "Converged after 31794 epochs\n",
      "Final theta0: 3.950827542856923\n",
      "Final theta1: 2.6834617400342577\n",
      "Converged after 30841 epochs\n",
      "Final theta0: 3.950585450348337\n",
      "Final theta1: 2.681476773923651\n",
      "Converged after 25472 epochs\n",
      "Final theta0: 3.9505862743390616\n",
      "Final theta1: 2.681476823984855\n",
      "Converged after 32328 epochs\n",
      "Final theta0: 3.9505845313165064\n",
      "Final theta1: 2.6814768828014066\n",
      "Converged after 31571 epochs\n",
      "Final theta0: 3.9508280452336035\n",
      "Final theta1: 2.683461840467764\n",
      "Converged after 27697 epochs\n",
      "Final theta0: 3.9505857184137483\n",
      "Final theta1: 2.6814766648648805\n",
      "Converged after 32717 epochs\n",
      "Final theta0: 3.9505857835964076\n",
      "Final theta1: 2.6814766736215616\n"
     ]
    }
   ],
   "source": [
    "mini_batch_iterations = []\n",
    "for a in range(len(theta0_rand_values)-1):\n",
    "    #batch_size = 10\n",
    "    #alpha_mini_batch = 0.0005\n",
    "    #convergence_threshold = 0.001\n",
    "    #epoch_f_mini_batch = 100000\n",
    "    theta0_mini_batch = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    theta1_mini_batch = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch = mini_batch_gradient_descent(x, y, theta0_mini_batch, theta1_mini_batch, alpha, batch_size, convergence_threshold, epoch_f_mini_batch)\n",
    "    mini_batch_iterations.append(len(val_theta0_mini_batch))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the number of average epochs required tested over different vector of theta values.\n",
    "also printing the average number of iterations required for convergence of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations for mini-batch gradient descent: 30148.11111111111\n",
      "Standard deviation of number of iterations for mini-batch gradient descent: 301481.1111111111\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of iterations for mini-batch gradient descent: {np.mean(mini_batch_iterations)}\")\n",
    "print(f\"Standard deviation of number of iterations for mini-batch gradient descent: {np.mean(mini_batch_iterations)*10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum number of epochs or iterations (average) are being required for the mini batch gradient descent, followed by full batch gradient descent and then the stochastic gradient descent.\n",
    "\n",
    "The reason for this can be the following:\n",
    "\n",
    "1) This model updated the values least number of times, as compared to the other two, as it updates only once per epoch, that is less frequency of updates.\n",
    "2) These updates can be stated as more accurate, as it takes the average of all the points, and then updates the value of theta vector.\n",
    "3) This results in a slow convergence of the loss function, as compared to the other two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization using Contour plots.\n",
    "\n",
    "Aim is to miminize the difference between the current loss which is based on the current (updated) value of theta, and the optimal loss.\n",
    "\n",
    "In this the z axis represent the difference, while x and y axis represent theta_0 ad theta_1 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done in File Final_plot_Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Question 4****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this, in the updation rule, there is an addition of **additional term**.\n",
    "\n",
    "This term is the product of **constant value momentum and the previous value of theta vector**.\n",
    "\n",
    "\n",
    "**Why do we need Momentum based gradient descent ?**\n",
    "\n",
    "1. In case there is a very gentle slope, then there wont be any significant change in the value of theta vector, and the vector gets stuck in that region.\n",
    "2. This is because the gradient in these regions were small.\n",
    "\n",
    "**How do we modify the algorithm.**\n",
    "\n",
    "1. As we go in a particular direction, our momentum in that particular direction has some impact. Similarly, if we are descending thru a slope of gradient descent, our momentum in that particular direction increases, and hence its better to include that momentum term along with the gradient term.\n",
    "\n",
    "2. This can be done, by incorporating some term from the previous update, to increase the movement in that particular direction.\n",
    "\n",
    "let v be the theta vector.\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{t+1} = \\mathbf{v}_t - \\text{update}_t\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\text{update}_t = \\gamma \\cdot \\text{update}_{t-1} + \\alpha \\cdot \\nabla \\mathbf{v}_t\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum based implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def true_gradient_momentum(theta0, theta1, x, y, alpha, convergence_threshold, loss_optimal, epoch_f, gamma):\n",
    "\n",
    "\n",
    "    v_theta0 = torch.zeros_like(theta0)\n",
    "    v_theta1 = torch.zeros_like(theta1)\n",
    "\n",
    "    val_theta0_momentum = [theta0.item()]\n",
    "    val_theta1_momentum = [theta1.item()]\n",
    "    loss_values_momentum = [loss_optimal.item()]\n",
    "\n",
    "    #As in the visualization part, we also need to plot the vector of grad and momentum, we require some more lists\n",
    "    grad_theta0_list = []\n",
    "    grad_theta1_list = []\n",
    "    momentum_theta0_list = []\n",
    "    momentum_theta1_list = []\n",
    "\n",
    "    epoch_vanilla = 0\n",
    "\n",
    "    while epoch_vanilla < epoch_f:\n",
    "        loss = ((y - theta1 * x - theta0) ** 2).mean()\n",
    "        loss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            grad_theta0_list.append(theta0.grad.item())\n",
    "            grad_theta1_list.append(theta1.grad.item())\n",
    "\n",
    "            v_theta0 = gamma * v_theta0 + alpha * theta0.grad\n",
    "            v_theta1 = gamma * v_theta1 + alpha * theta1.grad\n",
    "\n",
    "            momentum_theta0_list.append(v_theta0.item())\n",
    "            momentum_theta1_list.append(v_theta1.item())\n",
    "            \n",
    "            theta0 -= v_theta0\n",
    "            theta1 -= v_theta1\n",
    "        \n",
    "        val_theta0_momentum.append(theta0.detach().item())\n",
    "        val_theta1_momentum.append(theta1.detach().item())\n",
    "        loss_values_momentum.append(loss.item())\n",
    "\n",
    "        theta0.grad.zero_()\n",
    "        theta1.grad.zero_()\n",
    "\n",
    "        distance = torch.sqrt((theta0 - theta0_plot) ** 2 + (theta1 - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "\n",
    "        epoch_vanilla += 1\n",
    "\n",
    "    print(f\"Converged after {epoch_vanilla} epochs\")\n",
    "    print(f\"Final theta0: {theta0.item()}\")\n",
    "    print(f\"Final theta1: {theta1.item()}\")\n",
    "    return val_theta0_momentum, val_theta1_momentum, loss_values_momentum, grad_theta0_list, grad_theta1_list, momentum_theta0_list, momentum_theta1_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Value - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aveg_iteration_truegrad_momentum = []\n",
    "# alpha = 0.01\n",
    "# gamma = 0.9\n",
    "# convergence_threshold = 0.001\n",
    "# epoch_f = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 125 epochs\n",
      "Final theta0: 3.951430559158325\n",
      "Final theta1: 2.6819002628326416\n",
      "Converged after 148 epochs\n",
      "Final theta0: 3.950218439102173\n",
      "Final theta1: 2.6832096576690674\n",
      "Converged after 126 epochs\n",
      "Final theta0: 3.9511966705322266\n",
      "Final theta1: 2.681659698486328\n",
      "Converged after 134 epochs\n",
      "Final theta0: 3.9512546062469482\n",
      "Final theta1: 2.6833016872406006\n",
      "Converged after 145 epochs\n",
      "Final theta0: 3.9498283863067627\n",
      "Final theta1: 2.6829066276550293\n",
      "Converged after 126 epochs\n",
      "Final theta0: 3.9511818885803223\n",
      "Final theta1: 2.6817967891693115\n",
      "Converged after 147 epochs\n",
      "Final theta0: 3.9500367641448975\n",
      "Final theta1: 2.6831421852111816\n",
      "Converged after 134 epochs\n",
      "Final theta0: 3.951188087463379\n",
      "Final theta1: 2.6832616329193115\n",
      "Converged after 127 epochs\n",
      "Final theta0: 3.950914144515991\n",
      "Final theta1: 2.681494951248169\n",
      "Converged after 148 epochs\n",
      "Final theta0: 3.9502105712890625\n",
      "Final theta1: 2.6832289695739746\n"
     ]
    }
   ],
   "source": [
    "aveg_iteration_truegrad_momentum = []\n",
    "alpha = 0.01\n",
    "gamma = 0.9\n",
    "convergence_threshold = 0.001\n",
    "epoch_f = 20000\n",
    "\n",
    "theta0 = torch.tensor([2.5], requires_grad=True)\n",
    "theta1 = torch.tensor([2.5], requires_grad=True)\n",
    "val_theta0_momentum, val_theta1_momentum, loss_values_momentum, grad_theta0_list, grad_theta1_list, momentum_theta0_list, momentum_theta1_list = true_gradient_momentum(theta0, theta1, x, y, alpha, convergence_threshold, loss_optimal, epoch_f, gamma)\n",
    "aveg_iteration_truegrad_momentum.append(len(val_theta0_momentum)-2)\n",
    "\n",
    "data_momentum = pd.DataFrame({\n",
    "    'val_theta0': val_theta0_momentum,\n",
    "    'val_theta1': val_theta1_momentum,\n",
    "    'loss_values': loss_values_momentum,\n",
    "    'grad_theta0': grad_theta0_list + [0.0],\n",
    "    'grad_theta1': grad_theta1_list + [0.0],\n",
    "    'momentum_theta0': momentum_theta0_list + [0.0],\n",
    "    'momentum_theta1': momentum_theta1_list + [0.0]\n",
    "})\n",
    "\n",
    "data_momentum.to_csv('true_gradient_momentum.csv', index=False)\n",
    "\n",
    "for a in range(len(theta0_rand_values)-1):\n",
    "    theta0_momentum = torch.tensor([float(theta0_rand_values[a])], requires_grad=True)  # reinitialize for each iteration\n",
    "    theta1_momentum = torch.tensor([float(theta0_rand_values[a])], requires_grad=True)  # reinitialize for each iteration\n",
    "    \n",
    "    val_theta0_momentum, val_theta1_momentum, loss_values_momentum, grad_theta0_list, grad_theta1_list, momentum_theta0_list, momentum_theta1_list = true_gradient_momentum(theta0_momentum, theta1_momentum, x, y, alpha, convergence_threshold, loss_optimal, epoch_f, gamma)\n",
    "    \n",
    "    aveg_iteration_truegrad_momentum.append(len(val_theta0_momentum)-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random values using the list of theta values which are randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of epochs for true gradient momentum:  136.0\n",
      "average number of iterations for true gradient momentum:  136.0\n"
     ]
    }
   ],
   "source": [
    "print(\"average number of epochs for true gradient momentum: \", np.mean(aveg_iteration_truegrad_momentum))\n",
    "print(\"average number of iterations for true gradient momentum: \", np.mean(aveg_iteration_truegrad_momentum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying momentum based gradient descent for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sgd_momentum(theta0_sgd, theta1_sgd, x, y, alpha_sgd, convergence_threshold, loss_optimal, gamma_sgd):\n",
    "\n",
    "    global theta0_plot, theta1_plot\n",
    "\n",
    "    v_theta0_sgd = torch.zeros_like(theta0_sgd)\n",
    "    v_theta1_sgd = torch.zeros_like(theta1_sgd)\n",
    "\n",
    "    val_theta0_sgd = [theta0_sgd.item()]\n",
    "    val_theta1_sgd = [theta1_sgd.item()]\n",
    "    loss_values_sgd = [loss_optimal.item()]\n",
    "\n",
    "    grad_theta0_sgd_list = []\n",
    "    grad_theta1_sgd_list = []\n",
    "    momentum_theta0_sgd_list = []\n",
    "    momentum_theta1_sgd_list = []\n",
    "\n",
    "    epoch_sgd = 0\n",
    "    epoch_f_sgd = 20000 #limiting in case the values do not converge\n",
    "    flag = True\n",
    "\n",
    "    while epoch_sgd < epoch_f_sgd and flag:\n",
    "        indices = torch.randperm(x.size(0))\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        grad_theta0_epoch = []\n",
    "        grad_theta1_epoch = []\n",
    "        momentum_theta0_epoch = []\n",
    "        momentum_theta1_epoch = []\n",
    "        for i in range(len(x)):\n",
    "            prediction = theta1_sgd * x[i] + theta0_sgd\n",
    "            loss_sgd = (y[i] - prediction) ** 2\n",
    "            loss_sgd.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                grad_theta0_epoch.append(theta0_sgd.grad.item())\n",
    "                grad_theta1_epoch.append(theta1_sgd.grad.item())\n",
    "\n",
    "                v_theta0_sgd = gamma_sgd * v_theta0_sgd + alpha_sgd * theta0_sgd.grad\n",
    "                v_theta1_sgd = gamma_sgd * v_theta1_sgd + alpha_sgd * theta1_sgd.grad\n",
    "\n",
    "                momentum_theta0_epoch.append(v_theta0_sgd.item())\n",
    "                momentum_theta1_epoch.append(v_theta1_sgd.item())\n",
    "                \n",
    "                theta0_sgd -= v_theta0_sgd\n",
    "                theta1_sgd -= v_theta1_sgd\n",
    "\n",
    "                theta0_sgd.grad.zero_()\n",
    "                theta1_sgd.grad.zero_()\n",
    "\n",
    "        total_loss_sgd = ((y - (theta1_sgd * x + theta0_sgd)) ** 2).mean()\n",
    "\n",
    "        distance = torch.sqrt((theta0_sgd - theta0_plot) ** 2 + (theta1_sgd - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "\n",
    "        grad_theta0_sgd_list.append(np.mean(grad_theta0_epoch))\n",
    "        grad_theta1_sgd_list.append(np.mean(grad_theta1_epoch))\n",
    "        momentum_theta0_sgd_list.append(np.mean(momentum_theta0_epoch))\n",
    "        momentum_theta1_sgd_list.append(np.mean(momentum_theta1_epoch))\n",
    "\n",
    "        val_theta0_sgd.append(theta0_sgd.detach().item())\n",
    "        val_theta1_sgd.append(theta1_sgd.detach().item())\n",
    "        loss_values_sgd.append(total_loss_sgd.item())\n",
    "\n",
    "        epoch_sgd += 1\n",
    "\n",
    "    print(f\"Converged after {epoch_sgd} epochs\")\n",
    "    print(f\"Final theta0: {theta0_sgd.item()}\")\n",
    "    print(f\"Final theta1: {theta1_sgd.item()}\")\n",
    "    print(f\"Final loss: {loss_values_sgd[-1]}\")\n",
    "    return val_theta0_sgd, val_theta1_sgd, loss_values_sgd, grad_theta0_sgd_list, grad_theta1_sgd_list, momentum_theta0_sgd_list, momentum_theta1_sgd_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Value - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "aveg_iteration_sgd_momentum = []\n",
    "alpha_sgd = 0.0001\n",
    "gamma_sgd = 0.5\n",
    "convergence_threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 1161 epochs\n",
      "Final theta0: 3.9508306980133057\n",
      "Final theta1: 2.681478261947632\n",
      "Final loss: 0.5957545638084412\n"
     ]
    }
   ],
   "source": [
    "theta0_sgd = torch.tensor([2.5], requires_grad=True)\n",
    "theta1_sgd = torch.tensor([2.5], requires_grad=True)\n",
    "val_theta0_sgd, val_theta1_sgd, loss_values_sgd, grad_theta0_sgd_list, grad_theta1_sgd_list, momentum_theta0_sgd_list, momentum_theta1_sgd_list = sgd_momentum(theta0_sgd, theta1_sgd, x, y, alpha_sgd, convergence_threshold, loss_optimal, gamma_sgd)\n",
    "aveg_iteration_sgd_momentum.append(len(val_theta0_sgd)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sgd = pd.DataFrame({\n",
    "    'val_theta0_sgd': val_theta0_sgd,\n",
    "    'val_theta1_sgd': val_theta1_sgd,\n",
    "    'loss_values_sgd': loss_values_sgd,\n",
    "    'grad_theta0_sgd': grad_theta0_sgd_list + [0.0],\n",
    "    'grad_theta1_sgd': grad_theta1_sgd_list + [0.0],\n",
    "    'momentum_theta0_sgd': momentum_theta0_sgd_list + [0.0],\n",
    "    'momentum_theta1_sgd': momentum_theta1_sgd_list + [0.0]\n",
    "})\n",
    "\n",
    "data_sgd.to_csv('true_gradient_sgd_momentum.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 1585 epochs\n",
      "Final theta0: 3.950842545236206\n",
      "Final theta1: 2.6814807146534267\n",
      "Final loss: 0.5957545315252417\n",
      "Converged after 1269 epochs\n",
      "Final theta0: 3.9508425713343676\n",
      "Final theta1: 2.681480944417092\n",
      "Final loss: 0.5957545313864545\n",
      "Converged after 1664 epochs\n",
      "Final theta0: 3.951076126371595\n",
      "Final theta1: 2.683395388871288\n",
      "Final loss: 0.5957545298932395\n",
      "Converged after 1497 epochs\n",
      "Final theta0: 3.9508427989483974\n",
      "Final theta1: 2.681482794785603\n",
      "Final loss: 0.5957545302777555\n",
      "Converged after 1228 epochs\n",
      "Final theta0: 3.9508423292382124\n",
      "Final theta1: 2.681478973366601\n",
      "Final loss: 0.5957545325699848\n",
      "Converged after 1571 epochs\n",
      "Final theta0: 3.9508425597208707\n",
      "Final theta1: 2.6814808334250784\n",
      "Final loss: 0.5957545314539385\n",
      "Converged after 1652 epochs\n",
      "Final theta0: 3.9510764715538933\n",
      "Final theta1: 2.683398218344595\n",
      "Final loss: 0.5957545315896098\n",
      "Converged after 1340 epochs\n",
      "Final theta0: 3.950842860280933\n",
      "Final theta1: 2.6814833025958578\n",
      "Final loss: 0.595754529973355\n",
      "Converged after 1591 epochs\n",
      "Final theta0: 3.9508429726360244\n",
      "Final theta1: 2.6814842180522978\n",
      "Final loss: 0.5957545294258041\n"
     ]
    }
   ],
   "source": [
    "for a in range(len(theta0_rand_values)-1):\n",
    "    theta0_sgd = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    theta1_sgd = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    val_theta0_sgd, val_theta1_sgd, loss_values_sgd, grad_theta0_sgd_list, grad_theta1_sgd_list, momentum_theta0_sgd_list, momentum_theta1_sgd_list = sgd_momentum(theta0_sgd, theta1_sgd, x, y, alpha_sgd, convergence_threshold, loss_optimal, gamma_sgd)\n",
    "    aveg_iteration_sgd_momentum.append(len(val_theta0_sgd)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of epochs for SGD with momentum: 1455.8\n",
      "Average number of iterations for SGD with momentum: 58232.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of epochs for SGD with momentum: {np.mean(aveg_iteration_sgd_momentum)}\")\n",
    "print(f\"Average number of iterations for SGD with momentum: {np.mean(aveg_iteration_sgd_momentum)*40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini Batch Gradient with momentum updation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "alpha_mini_batch = 0.0001\n",
    "gamma_mini_batch = 0.9\n",
    "convergence_threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def minibatch_momentum(theta0_mini_batch, theta1_mini_batch , x, y, alpha_mini_batch, convergence_threshold, loss_optimal, gamma_mini_batch):\n",
    "    \n",
    "    v_theta0_mini_batch = torch.zeros_like(theta0_mini_batch)\n",
    "    v_theta1_mini_batch = torch.zeros_like(theta1_mini_batch)\n",
    "\n",
    "    val_theta0_mini_batch = [theta0_mini_batch.item()]\n",
    "    val_theta1_mini_batch = [theta1_mini_batch.item()]\n",
    "    loss_values_mini_batch = [loss_optimal.item()]\n",
    "\n",
    "    grad_theta0_minibatch = []\n",
    "    grad_theta1_minibatch = []\n",
    "    momentum_theta0_minibatch= []\n",
    "    momentum_theta1_minibatch = []\n",
    "\n",
    "    epoch_mini_batch = 0\n",
    "    epoch_f_mini_batch = 20000\n",
    "\n",
    "    while epoch_mini_batch < epoch_f_mini_batch:\n",
    "        indices = torch.randperm(x.size(0))\n",
    "        x = x[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "        grad_theta0_batch = []\n",
    "        grad_theta1_batch = []\n",
    "        momentum_theta0_batch = []\n",
    "        momentum_theta1_batch = []\n",
    "        num_batches = 0\n",
    "\n",
    "        for i in range(0, x.size()[0], batch_size):\n",
    "            batch_x = x[i: i + batch_size]\n",
    "            batch_y = y[i: i + batch_size]\n",
    "\n",
    "            loss_mini_batch = ((batch_y - theta1_mini_batch * batch_x - theta0_mini_batch) ** 2).mean()\n",
    "            loss_mini_batch.backward()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                v_theta0_mini_batch = gamma_mini_batch * v_theta0_mini_batch + alpha_mini_batch * theta0_mini_batch.grad\n",
    "                v_theta1_mini_batch = gamma_mini_batch * v_theta1_mini_batch + alpha_mini_batch * theta1_mini_batch.grad\n",
    "                \n",
    "                theta0_mini_batch -= v_theta0_mini_batch\n",
    "                theta1_mini_batch -= v_theta1_mini_batch\n",
    "\n",
    "                grad_theta0_batch.append(theta0_mini_batch.grad.item())\n",
    "                grad_theta1_batch.append(theta1_mini_batch.grad.item())\n",
    "                momentum_theta0_batch.append(v_theta0_mini_batch.item())\n",
    "                momentum_theta1_batch.append(v_theta1_mini_batch.item())\n",
    "\n",
    "            theta0_mini_batch.grad.zero_()\n",
    "            theta1_mini_batch.grad.zero_()\n",
    "            num_batches += 1\n",
    "\n",
    "        total_loss_mini_batch = ((y - (theta1_mini_batch * x + theta0_mini_batch)) ** 2).mean()\n",
    "\n",
    "        val_theta0_mini_batch.append(theta0_mini_batch.detach().item())\n",
    "        val_theta1_mini_batch.append(theta1_mini_batch.detach().item())\n",
    "        loss_values_mini_batch.append(total_loss_mini_batch.item())\n",
    "\n",
    "        grad_theta0_minibatch.append(np.mean(grad_theta0_batch))\n",
    "        grad_theta1_minibatch.append(np.mean(grad_theta1_batch))\n",
    "        momentum_theta0_minibatch.append(np.mean(momentum_theta0_batch))\n",
    "        momentum_theta1_minibatch.append(np.mean(momentum_theta1_batch))\n",
    "\n",
    "        distance = torch.sqrt((theta0_mini_batch - theta0_plot) ** 2 + (theta1_mini_batch - theta1_plot) ** 2)\n",
    "        if (abs(distance) < convergence_threshold):\n",
    "            break\n",
    "\n",
    "        epoch_mini_batch += 1\n",
    "\n",
    "    print(f\"Converged after {epoch_mini_batch} epochs\")\n",
    "    print(f\"Final theta0: {theta0_mini_batch.item()}\")\n",
    "    print(f\"Final theta1: {theta1_mini_batch.item()}\")\n",
    "    return val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch, grad_theta0_minibatch, grad_theta1_minibatch, momentum_theta0_minibatch, momentum_theta1_minibatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_iterations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 2395 epochs\n",
      "Final theta0: 3.9505698680877686\n",
      "Final theta1: 2.681480646133423\n"
     ]
    }
   ],
   "source": [
    "theta0_mini_batch = torch.tensor([2.5], requires_grad=True)\n",
    "theta1_mini_batch = torch.tensor([2.5], requires_grad=True)\n",
    "val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch, grad_theta0_minibatch, grad_theta1_minibatch, momentum_theta0_minibatch, momentum_theta1_minibatch = minibatch_momentum(theta0_mini_batch, theta1_mini_batch, x, y, alpha_mini_batch, convergence_threshold, loss_optimal, gamma_mini_batch)\n",
    "mini_batch_iterations.append(len(val_theta0_mini_batch)-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mini_batch = pd.DataFrame({\n",
    "    'val_theta0_mini_batch': val_theta0_mini_batch,\n",
    "    'val_theta1_mini_batch': val_theta1_mini_batch,\n",
    "    'loss_values_mini_batch': loss_values_mini_batch,\n",
    "    'grad_theta0_mini_batch': grad_theta0_minibatch + [0.0],\n",
    "    'grad_theta1_mini_batch': grad_theta1_minibatch + [0.0],\n",
    "    'momentum_theta0_mini_batch': momentum_theta0_minibatch + [0.0],\n",
    "    'momentum_theta1_mini_batch': momentum_theta1_minibatch + [0.0]\n",
    "})\n",
    "\n",
    "data_mini_batch.to_csv('true_gradient_mini_batch_momentum.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 3245 epochs\n",
      "Final theta0: 3.9505735402316686\n",
      "Final theta1: 2.681478358954157\n",
      "Converged after 2617 epochs\n",
      "Final theta0: 3.95057372222035\n",
      "Final theta1: 2.681479857134945\n",
      "Converged after 3161 epochs\n",
      "Final theta0: 3.9508154511854663\n",
      "Final theta1: 2.6834622423575163\n",
      "Converged after 3070 epochs\n",
      "Final theta0: 3.9505737560224827\n",
      "Final theta1: 2.6814801287831025\n",
      "Converged after 2536 epochs\n",
      "Final theta0: 3.9505736906708555\n",
      "Final theta1: 2.6814796032875114\n",
      "Converged after 3218 epochs\n",
      "Final theta0: 3.950573802625103\n",
      "Final theta1: 2.681480510825374\n",
      "Converged after 3139 epochs\n",
      "Final theta0: 3.950815393420549\n",
      "Final theta1: 2.6834617686331916\n",
      "Converged after 2757 epochs\n",
      "Final theta0: 3.9505735931290515\n",
      "Final theta1: 2.681478794737748\n",
      "Converged after 3256 epochs\n",
      "Final theta0: 3.950573607111548\n",
      "Final theta1: 2.6814789074234273\n"
     ]
    }
   ],
   "source": [
    "for a in range(len(theta0_rand_values)-1):\n",
    "    theta0_mini_batch = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    theta1_mini_batch = torch.tensor([theta0_rand_values[a]], requires_grad=True)\n",
    "    val_theta0_mini_batch, val_theta1_mini_batch, loss_values_mini_batch, grad_theta0_minibatch, grad_theta1_minibatch, momentum_theta0_minibatch, momentum_theta1_minibatch = minibatch_momentum(theta0_mini_batch, theta1_mini_batch, x, y, alpha_mini_batch, convergence_threshold, loss_optimal, gamma_mini_batch)\n",
    "    mini_batch_iterations.append(len(val_theta0_mini_batch)-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of epochs for Mini Batch with momentum: 2939.4\n",
      "Average number of iterations for Mini Batch with momentum: 29394.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of epochs for Mini Batch with momentum: {np.mean(mini_batch_iterations)}\")\n",
    "print(f\"Average number of iterations for Mini Batch with momentum: {np.mean(mini_batch_iterations)*10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of steps taken are least by true gradient by full batch gradient descent, followed by SGD with momnetum and then mini batch gradient descent with momentum.\n",
    "\n",
    "The epochs taken by mini batch are high as compared to the epochs required by other two"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_es335_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
